{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70ebff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93e9ec",
   "metadata": {},
   "source": [
    "## Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7677a7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning algorithms build a model based on sample data, \\n        known as training data, in order to make predictions or decisions \\n        without being explicitly programmed to do so.', 'Machine learning algorithms \\n        are used in a wide variety of applications, such as in medicine, email \\n        filtering, speech recognition, agriculture, and computer vision, where \\n        it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"\"\"Machine learning algorithms build a model based on sample data, \n",
    "        known as training data, in order to make predictions or decisions \n",
    "        without being explicitly programmed to do so. Machine learning algorithms \n",
    "        are used in a wide variety of applications, such as in medicine, email \n",
    "        filtering, speech recognition, agriculture, and computer vision, where \n",
    "        it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\"\"\"\n",
    "\n",
    "tokenized_text=sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c734d",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92c2d7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'learning', 'algorithms', 'build', 'a', 'model', 'based', 'on', 'sample', 'data', ',', 'known', 'as', 'training', 'data', ',', 'in', 'order', 'to', 'make', 'predictions', 'or', 'decisions', 'without', 'being', 'explicitly', 'programmed', 'to', 'do', 'so', '.', 'Machine', 'learning', 'algorithms', 'are', 'used', 'in', 'a', 'wide', 'variety', 'of', 'applications', ',', 'such', 'as', 'in', 'medicine', ',', 'email', 'filtering', ',', 'speech', 'recognition', ',', 'agriculture', ',', 'and', 'computer', 'vision', ',', 'where', 'it', 'is', 'difficult', 'or', 'unfeasible', 'to', 'develop', 'conventional', 'algorithms', 'to', 'perform', 'the', 'needed', 'tasks', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2fe45",
   "metadata": {},
   "source": [
    "## Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4210e9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 55 samples and 76 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_word)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26a94569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 8), ('to', 4)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6350b7",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "571a7a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shouldn', 've', 'now', \"you've\", 'had', 'more', 'the', 'y', 'where', 'each', 'should', 'other', 'that', 'her', 'not', 'a', 'he', 'how', 'our', 'very', 'o', 'out', 'm', 'these', \"you'll\", 'those', \"it's\", 'hadn', 'his', 'itself', 'from', 'did', 'you', \"you're\", 'too', 'mightn', \"shouldn't\", 'your', 'against', 'needn', 'between', 'with', 'during', 'down', 'himself', 'been', 'own', \"hadn't\", 'is', 'or', 'yours', 'of', 'all', 're', 'll', 'does', 'haven', 'couldn', \"mustn't\", 'am', \"isn't\", 'up', 'doing', 'until', 'over', \"didn't\", 'yourselves', 'if', 'because', 'then', \"haven't\", 'having', 'has', 'its', 'so', 'hasn', 'below', \"won't\", 'were', 'no', 'being', 'me', 'doesn', 'wouldn', 'again', 'why', 'shan', 'above', 'have', 'ain', 'ours', 'can', \"wouldn't\", \"that'll\", 'on', \"weren't\", 'will', \"she's\", 'do', 'just', \"needn't\", 's', 'it', 'off', \"doesn't\", 'ma', 'as', 'to', \"should've\", 'about', 'be', 'into', 'such', 'but', \"you'd\", \"shan't\", 'hers', \"hasn't\", 'further', 'in', \"couldn't\", \"mightn't\", 'than', 'i', 'by', 'd', 'themselves', 'mustn', 'was', 'which', 'their', 'isn', 'ourselves', 'before', 'didn', 'what', 'same', 'we', 'and', 'some', 'here', 'my', 'whom', 'myself', 'don', 'aren', 'she', 'there', 'herself', 'under', 'at', 'through', 'who', 'yourself', 'most', 'won', 'him', 'while', \"don't\", 'wasn', 'after', 'theirs', 'both', 'them', 't', 'weren', 'nor', 'they', 'are', 'only', \"wasn't\", 'few', 'once', 'when', \"aren't\", 'an', 'for', 'any', 'this'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/admin1/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f56317f",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c32a654b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['Machine', 'learning', 'algorithms', 'build', 'a', 'model', 'based', 'on', 'sample', 'data', ',', 'known', 'as', 'training', 'data', ',', 'in', 'order', 'to', 'make', 'predictions', 'or', 'decisions', 'without', 'being', 'explicitly', 'programmed', 'to', 'do', 'so', '.', 'Machine', 'learning', 'algorithms', 'are', 'used', 'in', 'a', 'wide', 'variety', 'of', 'applications', ',', 'such', 'as', 'in', 'medicine', ',', 'email', 'filtering', ',', 'speech', 'recognition', ',', 'agriculture', ',', 'and', 'computer', 'vision', ',', 'where', 'it', 'is', 'difficult', 'or', 'unfeasible', 'to', 'develop', 'conventional', 'algorithms', 'to', 'perform', 'the', 'needed', 'tasks', '.']\n",
      "\n",
      "Filterd Sentence: ['Machine', 'learning', 'algorithms', 'build', 'model', 'based', 'sample', 'data', ',', 'known', 'training', 'data', ',', 'order', 'make', 'predictions', 'decisions', 'without', 'explicitly', 'programmed', '.', 'Machine', 'learning', 'algorithms', 'used', 'wide', 'variety', 'applications', ',', 'medicine', ',', 'email', 'filtering', ',', 'speech', 'recognition', ',', 'agriculture', ',', 'computer', 'vision', ',', 'difficult', 'unfeasible', 'develop', 'conventional', 'algorithms', 'perform', 'needed', 'tasks', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Tokenized Sentence:\",tokenized_word)\n",
    "print(\"\\nFilterd Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7f9541",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8d54d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Machine', 'learning', 'algorithms', 'build', 'model', 'based', 'sample', 'data', ',', 'known', 'training', 'data', ',', 'order', 'make', 'predictions', 'decisions', 'without', 'explicitly', 'programmed', '.', 'Machine', 'learning', 'algorithms', 'used', 'wide', 'variety', 'applications', ',', 'medicine', ',', 'email', 'filtering', ',', 'speech', 'recognition', ',', 'agriculture', ',', 'computer', 'vision', ',', 'difficult', 'unfeasible', 'develop', 'conventional', 'algorithms', 'perform', 'needed', 'tasks', '.']\n",
      "\n",
      "Stemmed Sentence: ['machin', 'learn', 'algorithm', 'build', 'model', 'base', 'sampl', 'data', ',', 'known', 'train', 'data', ',', 'order', 'make', 'predict', 'decis', 'without', 'explicitli', 'program', '.', 'machin', 'learn', 'algorithm', 'use', 'wide', 'varieti', 'applic', ',', 'medicin', ',', 'email', 'filter', ',', 'speech', 'recognit', ',', 'agricultur', ',', 'comput', 'vision', ',', 'difficult', 'unfeas', 'develop', 'convent', 'algorithm', 'perform', 'need', 'task', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"\\nStemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0355f7b7",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e67ad7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: ['machin', 'learn', 'algorithm', 'build', 'model', 'base', 'sampl', 'data', ',', 'know', 'train', 'data', ',', 'order', 'make', 'predict', 'decis', 'without', 'explicitli', 'program', '.', 'machin', 'learn', 'algorithm', 'use', 'wide', 'varieti', 'applic', ',', 'medicin', ',', 'email', 'filter', ',', 'speech', 'recognit', ',', 'agricultur', ',', 'comput', 'vision', ',', 'difficult', 'unfeas', 'develop', 'convent', 'algorithm', 'perform', 'need', 'task', '.']\n",
      "\n",
      "Stemmed Word: ['machin', 'learn', 'algorithm', 'build', 'model', 'base', 'sampl', 'data', ',', 'known', 'train', 'data', ',', 'order', 'make', 'predict', 'decis', 'without', 'explicitli', 'program', '.', 'machin', 'learn', 'algorithm', 'use', 'wide', 'varieti', 'applic', ',', 'medicin', ',', 'email', 'filter', ',', 'speech', 'recognit', ',', 'agricultur', ',', 'comput', 'vision', ',', 'difficult', 'unfeas', 'develop', 'convent', 'algorithm', 'perform', 'need', 'task', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/admin1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "lemma_word_list = []\n",
    "\n",
    "for word in stemmed_words:\n",
    "    lemmat = lem.lemmatize(word,\"v\")\n",
    "    lemma_word_list.append(lemmat)\n",
    "    \n",
    "\n",
    "print(\"Lemmatized Word:\",lemma_word_list)\n",
    "print(\"\\nStemmed Word:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dcf13e",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dd97638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'learning', 'algorithms', 'build', 'a', 'model', 'based', 'on', 'sample', 'data', ',', 'known', 'as', 'training', 'data', ',', 'in', 'order', 'to', 'make', 'predictions', 'or', 'decisions', 'without', 'being', 'explicitly', 'programmed', 'to', 'do', 'so', '.', 'Machine', 'learning', 'algorithms', 'are', 'used', 'in', 'a', 'wide', 'variety', 'of', 'applications', ',', 'such', 'as', 'in', 'medicine', ',', 'email', 'filtering', ',', 'speech', 'recognition', ',', 'agriculture', ',', 'and', 'computer', 'vision', ',', 'where', 'it', 'is', 'difficult', 'or', 'unfeasible', 'to', 'develop', 'conventional', 'algorithms', 'to', 'perform', 'the', 'needed', 'tasks', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19a3be88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/admin1/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Machine', 'NN'),\n",
       " ('learning', 'VBG'),\n",
       " ('algorithms', 'JJ'),\n",
       " ('build', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('model', 'NN'),\n",
       " ('based', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('sample', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " (',', ','),\n",
       " ('known', 'VBN'),\n",
       " ('as', 'IN'),\n",
       " ('training', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('order', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('make', 'VB'),\n",
       " ('predictions', 'NNS'),\n",
       " ('or', 'CC'),\n",
       " ('decisions', 'NNS'),\n",
       " ('without', 'IN'),\n",
       " ('being', 'VBG'),\n",
       " ('explicitly', 'RB'),\n",
       " ('programmed', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('do', 'VB'),\n",
       " ('so', 'RB'),\n",
       " ('.', '.'),\n",
       " ('Machine', 'NNP'),\n",
       " ('learning', 'VBG'),\n",
       " ('algorithms', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('used', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('wide', 'JJ'),\n",
       " ('variety', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('applications', 'NNS'),\n",
       " (',', ','),\n",
       " ('such', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('in', 'IN'),\n",
       " ('medicine', 'NN'),\n",
       " (',', ','),\n",
       " ('email', 'NN'),\n",
       " ('filtering', 'NN'),\n",
       " (',', ','),\n",
       " ('speech', 'NN'),\n",
       " ('recognition', 'NN'),\n",
       " (',', ','),\n",
       " ('agriculture', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('computer', 'NN'),\n",
       " ('vision', 'NN'),\n",
       " (',', ','),\n",
       " ('where', 'WRB'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('difficult', 'JJ'),\n",
       " ('or', 'CC'),\n",
       " ('unfeasible', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('develop', 'VB'),\n",
       " ('conventional', 'JJ'),\n",
       " ('algorithms', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('perform', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('needed', 'JJ'),\n",
       " ('tasks', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "  \n",
    "\n",
    "nltk.pos_tag(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
